{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"K Nearest Neighbour.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"i0CczI75MQrp","colab_type":"code","outputId":"9ddbdbab-bec8-463c-9424-6c61055d7d4f","executionInfo":{"status":"ok","timestamp":1571151881535,"user_tz":300,"elapsed":768,"user":{"displayName":"bishwas ghimire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAX8psKN33HDIXghIsaXCds-GxzlrcG37xTD9LFaA=s64","userId":"02356194372121917490"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["#####################################\n","######## K nearest neighbour ########\n","\n","import numpy as np\n","import pandas as pd\n","import sklearn as sk\n","from sklearn.datasets import load_iris\n","\n","iris = load_iris()\n","X = iris.data\n","X = (X - X.mean(axis=0))/X.std(axis=0)\n","Y = iris.target\n","\n","K = 5\n","\n","\n","def one_hot_encode(Y):\n","  '''\n","  Implements one hot encoding. Given a categorical variable Y of length\n","  n and with p unique labels, returns a (n,p) shape boolean array \n","  where (i,j)th entry is True iff Y[i]==j\n","  '''\n","  labels = np.unique(Y)\n","  n,p = len(Y),len(labels)\n","  encoded = np.zeros((n,p), dtype = bool)\n","  for c,value in enumerate(labels):\n","    label_index = (Y == value) \n","    encoded[label_index,c] = True \n","  return labels, encoded  \n","labels,Y = one_hot_encode(Y)\n","\n","\n","#######################\n","##### My Solution #####\n","\n","#Create training and testing sets\n","#np.random.seed(42) # so we can reproduce the same results every time\n","n, p = X.shape\n","test_prop = 0.1 #proportion of data to reserve for testing\n","test_size = int(n*test_prop)\n","\n","prm  = np.random.permutation(n)\n","X = X[prm]\n","Y = Y[prm]\n","X_test = X[:test_size] #reserve top for testing \n","X_train = X[test_size:] #reserve remainder for training\n","Y_test = Y[:test_size] #reserve top for testing\n","Y_train = Y[test_size:] #use remainder for training\n","\n","\n","def distance(X_train,X_test):\n","  '''\n","  Use the L2 norm as a distance metric \n","  '''\n","  dX = X_train[:,np.newaxis,:] - X_test[np.newaxis,:,:] #add axes to make broadcasting work \n","  return np.linalg.norm(dX,axis=-1)\n","\n","def weights(d):\n","  '''\n","  Can use weights as a function of distance. Default is equals weights to each\n","  KNN\n","  '''\n","  W = np.ones(np.shape(d))\n","  return  W/np.sum(W, axis=0)\n","\n","d = distance(X_train,X_test)          \n","srt = np.argpartition(d,K,axis=0)[:K]   #bring k smallest distances from d to top and retursns their index\n","KNN = np.take_along_axis(d,srt,axis=0)  #retreive the distances of K nearest neighbors\n","W = np.expand_dims(weights(KNN),2)      #apply weights based on distances, expand dimensions to enable broadcasting later\n","\n","srt = srt[:,:,np.newaxis]               #add new axis at the end of sort to include the label values, k \n","Z = np.expand_dims(Y_train, 1)          #add new axis in the middle of Y_train to incorporate text index, j \n","Z = np.take_along_axis(Z,srt,axis=0)    #sort Z such that (i,j,k) entry is True if the ith KNN to jth test example has the label k \n","\n","Y_predicted = np.sum(W*Z, axis=0)\n","print(Y_predicted)\n","\n","def check_accuracy(Y_test,Y_predicted): \n","    '''\n","    Determines the accuracy of the model by comparing the predicted target \n","    variables with the actual test data\n","    '''\n","    idx_predicted = np.argmax(Y_predicted,axis=1) #extract the predicted indices in the model\n","    idx_test = np.argmax(Y_test,axis=1)           #extract the indices of the test data \n","    corr = (idx_predicted == idx_test)            #compare them to see how many were correct\n","    print('Model accuracy in percentage: {0:5.3f}'.format(np.sum(corr)/len(corr)*100))\n","    return \n","  \n","check_accuracy(Y_test,Y_predicted)\n","check_accuracy.__doc__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0.  0.  1. ]\n"," [1.  0.  0. ]\n"," [0.  1.  0. ]\n"," [0.  0.  1. ]\n"," [0.  0.  1. ]\n"," [1.  0.  0. ]\n"," [1.  0.  0. ]\n"," [0.  0.  1. ]\n"," [0.  0.8 0.2]\n"," [0.  1.  0. ]\n"," [0.  0.8 0.2]\n"," [0.  0.  1. ]\n"," [1.  0.  0. ]\n"," [0.  0.  1. ]\n"," [0.  1.  0. ]]\n","Model accuracy in percentage: 93.333\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\n    Determines the accuracy of the model by comparing the predicted target \\n    variables with the actual test data\\n    '"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"I-eErI1RulZ1","colab_type":"text"},"source":["expand_dims\n","\n","arg_partition\n","\n","take_along_axis\n","\n","linalg.norm"]},{"cell_type":"markdown","metadata":{"id":"rldIK6WDCZmY","colab_type":"text"},"source":["One hot encoding: When we have many levels in your categorical variable, it's easier to find the unique labels and construct one column for each label with boolean (True or False) values. "]},{"cell_type":"code","metadata":{"id":"-6ERh9vtBhQ3","colab_type":"code","outputId":"c9ae418d-a8d0-4683-a76f-5e37c4e92d53","executionInfo":{"status":"ok","timestamp":1570739338470,"user_tz":300,"elapsed":346,"user":{"displayName":"bishwas ghimire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAX8psKN33HDIXghIsaXCds-GxzlrcG37xTD9LFaA=s64","userId":"02356194372121917490"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Z.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, 15, 3)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"DJ-0rxCoNjyw","colab_type":"code","outputId":"0448d743-f44b-466a-8627-7996ac7e6d39","executionInfo":{"status":"ok","timestamp":1570663300411,"user_tz":300,"elapsed":698,"user":{"displayName":"bishwas ghimire","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAX8psKN33HDIXghIsaXCds-GxzlrcG37xTD9LFaA=s64","userId":"02356194372121917490"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["#KNN from scikit learn\n","from sklearn.neighbors import KNeighborsClassifier\n","model = KNeighborsClassifier(n_neighbors=5)\n","model.fit(X_train, Y_train)\n","Y_model = model.predict(X_test)\n","print(Y_model)\n","from sklearn.metrics import accuracy_score\n","accuracy_score(Y_test, Y_model)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[False  True False]\n"," [False False  True]\n"," [ True False False]\n"," [False False  True]\n"," [False  True False]\n"," [False False  True]\n"," [ True False False]\n"," [ True False False]\n"," [False False  True]\n"," [False  True False]\n"," [False False  True]\n"," [ True False False]\n"," [ True False False]\n"," [ True False False]\n"," [False  True False]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"TGdkU9-NOULh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}